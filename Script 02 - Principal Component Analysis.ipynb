{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "\n",
    "<br><h2>Script 02 | Principal Component Analysis</h2>\n",
    "<br>\n",
    "Written by Chase Kusterer<br>\n",
    "<a href=\"https://github.com/chase-kusterer\">GitHub</a> | <a href=\"https://www.linkedin.com/in/kusterer/\">LinkedIn</a>\n",
    "<br><br><br>\n",
    "\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<h2>Part I: Introduction and Preparation</h2><br>\n",
    "\n",
    "<strong>A Note on the Dataset</strong><br>\n",
    "The dataset in this script represents the annual spending of a subset of the top customers for Apprentice Chef, Inc. The monetary units are unknown, and the demographic information related to each client is as follows:<br><br><br>\n",
    "<u>Channel</u><br>\n",
    "\n",
    "1. Online\n",
    "2. Mobile App\n",
    "\n",
    "<br>\n",
    "<u>Region</u><br>\n",
    "\n",
    "1. Alameda\n",
    "2. San Francisco\n",
    "3. Contra Costa\n",
    "\n",
    "<br><br>\n",
    "Run the following code to import necessary packages, load data, and set display options. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# importing packages\n",
    "########################################\n",
    "import numpy             as np                   # mathematical essentials\n",
    "import pandas            as pd                   # data science essentials\n",
    "import matplotlib.pyplot as plt                  # fundamental data visualization\n",
    "import seaborn           as sns                  # enhanced visualization\n",
    "from sklearn.preprocessing import StandardScaler # standard scaler\n",
    "from sklearn.decomposition import PCA            # pca\n",
    "\n",
    "\n",
    "########################################\n",
    "# loading data and setting display options\n",
    "########################################\n",
    "# loading data\n",
    "customers_df = pd.read_excel(io = './datasets/top_customers.xlsx')\n",
    "\n",
    "\n",
    "# setting print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<strong>User-Defined Functions</strong><br>\n",
    "Run the following code to load the user-defined functions used throughout this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     3,
     48
    ],
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# scree_plot\n",
    "########################################\n",
    "def scree_plot(pca_object, export = False):\n",
    "    \"\"\"\n",
    "    Visualizes a scree plot from a pca object.\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    pca_object | A fitted pca object\n",
    "    export     | Set to True if you would like to save the scree plot to the\n",
    "               | current working directory (default: False)\n",
    "    \"\"\"\n",
    "    # building a scree plot\n",
    "\n",
    "    # setting plot size\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    features = range(pca_object.n_components_)\n",
    "\n",
    "\n",
    "    # developing a scree plot\n",
    "    plt.plot(features,\n",
    "             pca_object.explained_variance_ratio_,\n",
    "             linewidth = 2,\n",
    "             marker = 'o',\n",
    "             markersize = 10,\n",
    "             markeredgecolor = 'black',\n",
    "             markerfacecolor = 'grey')\n",
    "\n",
    "\n",
    "    # setting more plot options\n",
    "    plt.title('Scree Plot')\n",
    "    plt.xlabel('PCA feature')\n",
    "    plt.ylabel('Explained Variance')\n",
    "    plt.xticks(features)\n",
    "\n",
    "    if export == True:\n",
    "    \n",
    "        # exporting the plot\n",
    "        plt.savefig('./analysis_images/top_customers_correlation_scree_plot.png')\n",
    "        \n",
    "    # displaying the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "########################################\n",
    "# unsupervised_scaler\n",
    "########################################\n",
    "def scaler(df):\n",
    "    \"\"\"\n",
    "    Standardizes a dataset (mean = 0, variance = 1). Returns a new DataFrame.\n",
    "    Requires sklearn.preprocessing.StandardScaler()\n",
    "    \n",
    "    PARAMETERS\n",
    "    ----------\n",
    "    df     | DataFrame to be used for scaling\n",
    "    \"\"\"\n",
    "\n",
    "    # INSTANTIATING a StandardScaler() object\n",
    "    scaler = StandardScaler(copy = True)\n",
    "\n",
    "\n",
    "    # FITTING the scaler with the data\n",
    "    scaler.fit(df)\n",
    "\n",
    "\n",
    "    # TRANSFORMING our data after fit\n",
    "    x_scaled = scaler.transform(df)\n",
    "\n",
    "    \n",
    "    # converting scaled data into a DataFrame\n",
    "    new_df = pd.DataFrame(x_scaled)\n",
    "\n",
    "\n",
    "    # reattaching column names\n",
    "    new_df.columns = list(df.columns)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h4>a) Write code to check information about non-missing values and data types for each column.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# checking information about each feature\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# checking information about each feature\n",
    "customers_df.info(verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h4>b) Write code to check descriptive statistics on the numeric features of the dataset.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# descriptive statistics about each numeric feature\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# descriptive statistics about each numeric feature\n",
    "customers_df.describe(include = 'number').round(decimals = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# value counts for channel and region\n",
    "print(f\"\"\"\\\n",
    "Channel\n",
    "-------\n",
    "{customers_df['Channel'].value_counts(normalize=False).to_string(buf=None)}\n",
    "\n",
    "\n",
    "Region\n",
    "------\n",
    "{customers_df['Region'].value_counts(normalize=False).to_string(buf=None)}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<strong>Run the following code to generate histograms for each of the features in the dataset.</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# setting figure size\n",
    "fig, ax = plt.subplots(figsize = (12, 8))\n",
    "ax.remove()\n",
    "\n",
    "# initializing a counter\n",
    "count = 0\n",
    "\n",
    "\n",
    "# looping to create visualizations\n",
    "for col in customers_df:\n",
    "\n",
    "    # condition to break\n",
    "    if count == 8:\n",
    "        break\n",
    "    \n",
    "    # increasing count\n",
    "    count += 1\n",
    "    \n",
    "    # preparing histograms\n",
    "    plt.subplot(3, 3, count)\n",
    "    sns.histplot(x = customers_df[col],)\n",
    "\n",
    "\n",
    "# formatting, saving, and displaying the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<strong>Datasets with Features for Different Purposes</strong><br>\n",
    "Notice from the outputs above that the dataset contains demographic data (channel and region) as well purchasing data (spending per category). In unsupervised learning, feature types such as these should not be used together in the same algorithm. Demographic data is extremely different from purchasing data, and their concatenation would bias the results of an analysis. Instead, if a problem requires unsupervised learning and demographic data is present in the dataset, a best practice is to remove the demographic data before building an algorithm. Later, demographic data can be used to compare results.<br><br><br>\n",
    "<strong>PCA and Scaling</strong><br>\n",
    "As with KNN, explanatory variables should be scaled before developing a principal component analysis algorithm.<br><br><br>\n",
    "<h4>c) Complete the following in the code below:</h4>\n",
    "\n",
    "* drop demographic data and the non-logarithmic features. Store the result as purchase_behavior\n",
    "* instantiate a StandardScaler( ) object\n",
    "* fit the scaler object to purchase_behavior\n",
    "* transform purchase_behavior using the scaler object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# removing demographic data\n",
    "purchases_df = customers_df.drop(['Channel', 'Region'], axis = 1)\n",
    "\n",
    "\n",
    "# scaling features before correlation analysis\n",
    "purchases_scaled = scaler(df = _____)\n",
    "\n",
    "\n",
    "# checking pre- and post-scaling variance\n",
    "print(np.var(customers_df), '\\n\\n')\n",
    "print(np.var(purchases_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# removing demographic data\n",
    "purchases_df = customers_df.drop(['Channel', 'Region'], axis = 1)\n",
    "\n",
    "\n",
    "# scaling features before correlation analysis\n",
    "purchases_scaled = scaler(df = purchases_df)\n",
    "\n",
    "\n",
    "# checking pre- and post-scaling variance\n",
    "print(np.var(customers_df), '\\n\\n')\n",
    "print(np.var(purchases_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h4>d) Fill in the blanks below to develop a correlation heatmap of the scaled purchasing features.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# setting plot size\n",
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "\n",
    "\n",
    "# developing a correlation matrix object\n",
    "df_corr = _____._____.round(decimals = 2)\n",
    "\n",
    "\n",
    "# creating a correlation heatmap\n",
    "sns.heatmap(data   = _____,\n",
    "            cmap   = 'Blues',\n",
    "            square = True,\n",
    "            annot  = True)\n",
    "\n",
    "\n",
    "# rendering the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# setting plot size\n",
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "\n",
    "\n",
    "# developing a correlation matrix object\n",
    "df_corr = purchases_scaled.corr(method = 'pearson').round(decimals = 2)\n",
    "\n",
    "\n",
    "# creating a correlation heatmap\n",
    "sns.heatmap(data   = df_corr,\n",
    "            cmap   = 'Blues',\n",
    "            square = True,\n",
    "            annot  = True)\n",
    "\n",
    "\n",
    "# rendering the heatmap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "Notice that only a few (Pearson) correlations have an absolute value above 0.50. This makes the dataset a good candidate for PCA. As such, we may be able to explain a high degree of variance with a small number of principal components.<br><br>\n",
    "\n",
    "<h2>Part II: Principal Component Analysis</h2><br>\n",
    "Principal component analysis is primarily conducted in three situations:<br>\n",
    "\n",
    "<u>Correlated Explanatory Variables</u><br>\n",
    "Model building with correlated explanatory variables is a violation of one of the key assumptions of generalized linear models.<br><br>\n",
    "\n",
    "<u>Dimensionality Reduction</u><br>\n",
    "This is commonly conducted when a dataset has a large amount of explanatory variables (i.e., every unique click a user has made on a website). Techniques like PCA allow features to be transformed into principal components, (potentially) reducing the number of features needed to explain a high degree of variance.<br><br>\n",
    "\n",
    "<u>Latent Trait Exploration</u><br>\n",
    "Understanding factors that cannot be measured directly through measurable constructs.<br><br><br>\n",
    "<strong>Determining the Number of Principal Components</strong><br>A common heuristic is to include enough principal components to explain at least 80% of the variance in a dataset.\n",
    "<br><br>\n",
    "\n",
    "<h4>a) Complete the code below.</h4>\n",
    "Complete the code to instantiate, fit, and transform a PCA model with no limits to its number of principal components. Make sure to use the scaled dataset for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING a PCA object with no limit to principal components\n",
    "pca = _____(n_components = None,\n",
    "            random_state = 702)\n",
    "\n",
    "\n",
    "# FITTING and TRANSFORMING the scaled data\n",
    "customer_pca = _____._____(_____)\n",
    "\n",
    "\n",
    "# comparing dimensions of each DataFrame\n",
    "print(\"Original shape:\", purchases_scaled.shape)\n",
    "print(\"PCA shape     :\", customer_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING a PCA object with no limit to principal components\n",
    "pca = PCA(n_components = None,\n",
    "          random_state = 702)\n",
    "\n",
    "\n",
    "# FITTING and TRANSFORMING the scaled data\n",
    "customer_pca = pca.fit_transform(purchases_scaled)\n",
    "\n",
    "\n",
    "# comparing dimensions of each DataFrame\n",
    "print(\"Original shape:\", purchases_scaled.shape)\n",
    "print(\"PCA shape     :\", customer_pca.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h2>Part III: Evaluating PCA Algorithms</h2><br>\n",
    "As can be observed from above, the shape of the data did not change. However, the original DataFrame contains features, whereas the new DataFrame contains principal components. Before analyzing the factor loadings of each principal component, it is important to check each component's explained variance ratio. Also note that the sum of all explained variance ratios should sum to 1.0.<br><br><br>\n",
    "<h4>a) Complete the loop to print out each explained variance ratio.</h4>\n",
    "Write code to loop over each principal component, printing its component number as well as its explained variance ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# component number counter\n",
    "component_number = 0\n",
    "\n",
    "# looping over each principal component\n",
    "_____ variance _____ pca._____:\n",
    "    component_number += _____\n",
    "    \n",
    "    print(f\"PC {_____}: {_____.round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# component number counter\n",
    "component_number = 0\n",
    "\n",
    "\n",
    "# looping over each principal component\n",
    "for variance in pca.explained_variance_ratio_:\n",
    "    component_number += 1\n",
    "    print(f\"PC {component_number}: {variance.round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# printing the sum of all explained variance ratios\n",
    "print(pca.explained_variance_ratio_.sum(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<strong>Scree Plots</strong><br>\n",
    "One useful tool to visualize the explained variance of each principal component is the scree plot. Our goal in analyzing this plot is to look for a point where there is a drop in the marginal return of explained variance. In other words, we are looking for an \"elbow\" in the plot, where the line connecting each principal component becomes less steep.<br><br>\n",
    "<h4>c) Call the scree_plot function on the PCA object.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# calling the scree_plot function\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# calling the scree_plot function\n",
    "scree_plot(pca_object = pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h2>Part IV: Interpreting Principal Components and Latent Traits</h2><br>\n",
    "Principal components are essentially \"bundles\" of various parts of the explanatory variables that were used when building an algorithm. Note that each principal component is not directly measurable, but can be measured indirectly by analyzing its <strong>factor loadings</strong>. In other words, we can interpret the meaning of each principal component by looking into which features are strongly correlated with it.<br><br>\n",
    "Run the following code and analyze the resulting correlation map between the original features and the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# setting plot size\n",
    "fig, ax = plt.subplots(figsize = (12, 12))\n",
    "\n",
    "\n",
    "# developing a PC to feature heatmap\n",
    "sns.heatmap(pca.components_, \n",
    "            cmap = 'coolwarm',\n",
    "            square = True,\n",
    "            annot = True,\n",
    "            linewidths = 0.1,\n",
    "            linecolor = 'black')\n",
    "\n",
    "\n",
    "# setting more plot options\n",
    "plt.yticks([0, 1, 2, 3, 4, 5],\n",
    "           [\"PC 1\", \"PC 2\", \"PC 3\", \"PC 4\", \"PC 5\", \"PC 6\"])\n",
    "\n",
    "plt.xticks(range(0, 6),\n",
    "           purchases_scaled.columns,\n",
    "           rotation=60,\n",
    "           ha='left')\n",
    "\n",
    "plt.xlabel(xlabel = \"Feature\")\n",
    "plt.ylabel(ylabel = \"Principal Component\")\n",
    "\n",
    "\n",
    "# displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "Each observation in the dataset is a customer of Apprentice Chef, Inc. Therefore, each principal component can be thought of as a behavioral scale. Naming scales is subjective and often benefits from working with others.<br><br><br>\n",
    "<h4>a) Analyze the PC factor loadings.</h4>\n",
    "Run the following code. With your team, analyze the factor loadings and develop a scale for each principal component. When finished, rename the columns of the table with your team's scale names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# transposing pca components\n",
    "factor_loadings_df = pd.DataFrame(np.transpose(pca.components_.round(decimals = 2)))\n",
    "\n",
    "\n",
    "# naming rows as original features\n",
    "factor_loadings_df = factor_loadings_df.set_index(purchases_scaled.columns)\n",
    "\n",
    "\n",
    "# checking the result\n",
    "print(factor_loadings_df)\n",
    "\n",
    "\n",
    "# saving to Excel\n",
    "factor_loadings_df.to_excel(excel_writer = 'customer_factor_loadings.xlsx',\n",
    "                            index        = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# naming each principal component\n",
    "factor_loadings_df._____ = _____\n",
    "\n",
    "\n",
    "# checking the result\n",
    "factor_loadings_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# naming each principal component\n",
    "factor_loadings_df.columns = ['Protein Over Vitamins', # - Vegan, - Veg, - Indian\n",
    "                              'Sleeptime Bliss',       # - Med, - ME, - Wine\n",
    "                              'Palate Practicality',   # + Med, - Wine\n",
    "                              '3',                     # after elbow\n",
    "                              '4',                     # after elbow\n",
    "                              '5']                     # after elbow\n",
    "\n",
    "\n",
    "# checking the result\n",
    "factor_loadings_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<strong>Customer-Level Personas</strong><br>\n",
    "Earlier in this script we instantiated, fit, and transformed the dataset's original features into principal components:<br><br>\n",
    "\n",
    "~~~\n",
    "# FITTING and TRANSFORMING the scaled data\n",
    "customer_pca = pca.fit_transform(purchases_scaled)\n",
    "~~~\n",
    "\n",
    "<br>\n",
    "Now that we have developed personas, we can analyze how much each customer fits into each group. Run the following code to view the personas and factor loadings for each customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# converting into a DataFrame \n",
    "customer_pca = pd.DataFrame(customer_pca)\n",
    "\n",
    "\n",
    "# renaming columns\n",
    "customer_pca.columns = factor_loadings_df.columns\n",
    "\n",
    "\n",
    "# checking results\n",
    "customer_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "Digging deeper into the DataFrame above can unearth key findings and market opportunities. <strong>This is something expected of you on your final project.</strong> As an example, if we were exploring the market potential for customers with a standard deviation of one or above in the Healthfood Heroes persona, we could do so through subsetting, as in the following code. Try this on other personas and enjoy the exploration :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# exploring customers in the Healthfood Heroes persona\n",
    "len(customer_pca['Protein Over Vitamins'][customer_pca['Protein Over Vitamins'] > 1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h2>Part VI: Reducing to Relevant Principal Components</h2><br>\n",
    "In this example, we will assume three PCs is a reasonable number based on the elbow in the scree plot. Also note that it would have been reasonable to retain enough PCs so that the cumulative explained variance ratio is greater than or equal to 0.80. Note that we do not need to rerun the scree plot after completing this step.\n",
    "<br>\n",
    "<h4>a) Complete the code to develop a new PCA model.</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING a new model using the first three principal components\n",
    "pca_3 = _____(_____,\n",
    "            _____ = 702)\n",
    "\n",
    "\n",
    "# FITTING and TRANSFORMING the purchases_scaled\n",
    "customer_pca_3 = _____._____(_____)\n",
    "\n",
    "\n",
    "# calling the scree_plot function\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# INSTANTIATING a new model using the first three principal components\n",
    "pca_3 = PCA(n_components = 3,\n",
    "            random_state = 702)\n",
    "\n",
    "\n",
    "# FITTING and TRANSFORMING the purchases_scaled\n",
    "customer_pca_3 = pca_3.fit_transform(purchases_scaled)\n",
    "\n",
    "\n",
    "# calling the scree_plot function\n",
    "scree_plot(pca_object = pca_3,\n",
    "           export     = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<strong>OPTIONAL STEP</strong><br>Run the following code to compare the variance of the unlimited PCA model with the variance of the reduced PCA model. We are doing this in this script simply to show that the explained variance for each principal component does not change after dropping smaller PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "####################\n",
    "### Max PC Model ###\n",
    "####################\n",
    "# transposing pca components (pc = MAX)\n",
    "factor_loadings = pd.DataFrame(np.transpose(pca.components_))\n",
    "\n",
    "\n",
    "# naming rows as original features\n",
    "factor_loadings = factor_loadings.set_index(purchases_scaled.columns)\n",
    "\n",
    "\n",
    "##################\n",
    "### 3 PC Model ###\n",
    "##################\n",
    "# transposing pca components (pc = 3)\n",
    "factor_loadings_3 = pd.DataFrame(np.transpose(pca_3.components_))\n",
    "\n",
    "\n",
    "# naming rows as original features\n",
    "factor_loadings_3 = factor_loadings_3.set_index(purchases_scaled.columns)\n",
    "\n",
    "\n",
    "# checking the results\n",
    "print(f\"\"\"\n",
    "MAX Components Factor Loadings\n",
    "------------------------------\n",
    "{factor_loadings.round(decimals = 2)}\n",
    "\n",
    "\n",
    "3 Components Factor Loadings\n",
    "------------------------------\n",
    "{factor_loadings_3.round(decimals = 2)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" /><br>\n",
    "\n",
    "<h4>b) Analyze and name each principal component based on its factor loading.</h4>\n",
    "\n",
    "In this step, make sure to develop a story behind what each PC name represents. This is an ideal method for bridging the gap between the technical and non-technical people you are working with. Remember, by doing a good job here you are putting analytics at the forefront of strategic decision making, which is a great way to boost your value within an organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "outputs": [],
   "source": [
    "# naming each principal component\n",
    "factor_loadings_3._____ = ['Protein Over Vitamins', # - Vegan, - Veg, - Indian\n",
    "                           'Sleeptime Bliss',       # - Med, - ME, - Wine\n",
    "                           'Palate Practicality',]  # Med, no Wine\n",
    "\n",
    "\n",
    "# checking the result\n",
    "_____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "# naming each principal component\n",
    "factor_loadings_3.columns = ['Protein Over Vitamins', # - Vegan, - Veg, - Indian\n",
    "                             'Sleeptime Bliss',       # - Med, - ME, - Wine\n",
    "                             'Palate Practicality',]  # Med, no Wine\n",
    "\n",
    "\n",
    "# checking the result\n",
    "factor_loadings_3.round(decimals = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# converting customer-level data into DataFrame\n",
    "customer_pca_3 = pd.DataFrame(customer_pca_3)\n",
    "\n",
    "\n",
    "# renaming customer-level data\n",
    "customer_pca_3.columns = list(factor_loadings_3.columns)\n",
    "\n",
    "\n",
    "# checking factor loadings per customer\n",
    "customer_pca_3.head(n = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "\n",
    "~~~\n",
    "\n",
    "\n",
    " ,--.-,,-,--,             .-._            _,---.                                        \n",
    "/==/  /|=|  |.--.-. .-.-./==/ \\  .-._ _.='.'-,  \\  .-.,.---.  ,--.-.  .-,--.            \n",
    "|==|_ ||=|, /==/ -|/=/  ||==|, \\/ /, /==.'-     / /==/  `   \\/==/- / /=/_ /             \n",
    "|==| ,|/=| _|==| ,||=| -||==|-  \\|  /==/ -   .-' |==|-, .=., \\==\\, \\/=/. /              \n",
    "|==|- `-' _ |==|- | =/  ||==| ,  | -|==|_   /_,-.|==|   '='  /\\==\\  \\/ -/               \n",
    "|==|  _     |==|,  \\/ - ||==| -   _ |==|  , \\_.' )==|- ,   .'  |==|  ,_/                \n",
    "|==|   .-. ,\\==|-   ,   /|==|  /\\ , \\==\\-  ,    (|==|_  . ,'.  \\==\\-, /                 \n",
    "/==/, //=/  /==/ , _  .' /==/, | |- |/==/ _  ,  //==/  /\\ ,  ) /==/._/                  \n",
    "`--`-' `-`--`--`..---'   `--`./  `--``--`------' `--`-`--`--'  `--`-`                   \n",
    "     _,---.     _,.---._                                                                \n",
    "  .-`.' ,  \\  ,-.' , -  `.   .-.,.---.                                                  \n",
    " /==/_  _.-' /==/_,  ,  - \\ /==/  `   \\                                                 \n",
    "/==/-  '..-.|==|   .=.     |==|-, .=., |                                                \n",
    "|==|_ ,    /|==|_ : ;=:  - |==|   '='  /                                                \n",
    "|==|   .--' |==| , '='     |==|- ,   .'                                                 \n",
    "|==|-  |     \\==\\ -    ,_ /|==|_  . ,'.                                                 \n",
    "/==/   \\      '.='. -   .' /==/  /\\ ,  )                                                \n",
    "`--`---'        `--`--''   `--`-`--`--'                                                 \n",
    "   ,-,--.                _,.----.    _,.----.       ,----.    ,-,--.    ,-,--.   .=-.-. \n",
    " ,-.'-  _\\ .--.-. .-.-..' .' -   \\ .' .' -   \\   ,-.--` , \\ ,-.'-  _\\ ,-.'-  _\\ /==/_ / \n",
    "/==/_ ,_.'/==/ -|/=/  /==/  ,  ,-'/==/  ,  ,-'  |==|-  _.-`/==/_ ,_.'/==/_ ,_.'|==|, |  \n",
    "\\==\\  \\   |==| ,||=| -|==|-   |  .|==|-   |  .  |==|   `.-.\\==\\  \\   \\==\\  \\   |==|  |  \n",
    " \\==\\ -\\  |==|- | =/  |==|_   `-' \\==|_   `-' \\/==/_ ,    / \\==\\ -\\   \\==\\ -\\  /==/. /  \n",
    " _\\==\\ ,\\ |==|,  \\/ - |==|   _  , |==|   _  , ||==|    .-'  _\\==\\ ,\\  _\\==\\ ,\\ `--`-`   \n",
    "/==/\\/ _ ||==|-   ,   |==\\.       |==\\.       /|==|_  ,`-._/==/\\/ _ |/==/\\/ _ | .=.     \n",
    "\\==\\ - , //==/ , _  .' `-.`.___.-' `-.`.___.-' /==/ ,     /\\==\\ - , /\\==\\ - , /:=; :    \n",
    " `--`---' `--`..---'                           `--`-----``  `--`---'  `--`---'  `=`                                                                      \n",
    "\n",
    "\n",
    "\n",
    "~~~\n",
    "\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
